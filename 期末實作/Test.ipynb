{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNop/wACT7FS5hf9u3cUIjS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrXv2BhNrtpk","executionInfo":{"status":"ok","timestamp":1718418002827,"user_tz":-480,"elapsed":31076,"user":{"displayName":"杏仁Kimi","userId":"06196007917832630822"}},"outputId":"5a0f9770-1d68-40e9-8ce0-c6962eb48f62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 22 images belonging to 4 classes.\n","Found 4 images belonging to 4 classes.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"]},{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","9406464/9406464 [==============================] - 0s 0us/step\n","Epoch 1/10\n","2/2 [==============================] - 5s 1s/step - loss: 1.3122 - val_loss: 1.2456\n","Epoch 2/10\n","2/2 [==============================] - 1s 233ms/step - loss: 0.8739 - val_loss: 0.9254\n","Epoch 3/10\n","2/2 [==============================] - 2s 2s/step - loss: 0.5033 - val_loss: 0.7835\n","Epoch 4/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.3376 - val_loss: 0.6486\n","Epoch 5/10\n","2/2 [==============================] - 1s 198ms/step - loss: 0.2701 - val_loss: 0.5119\n","Epoch 6/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.1605 - val_loss: 0.3992\n","Epoch 7/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.1114 - val_loss: 0.2998\n","Epoch 8/10\n","2/2 [==============================] - 1s 198ms/step - loss: 0.0900 - val_loss: 0.2229\n","Epoch 9/10\n","2/2 [==============================] - 1s 208ms/step - loss: 0.0651 - val_loss: 0.1678\n","Epoch 10/10\n","2/2 [==============================] - 2s 2s/step - loss: 0.0439 - val_loss: 0.1284\n","1/1 [==============================] - 1s 1s/step\n","Lion 0.9936702251434326\n"]}],"source":["from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import Model, layers\n","from PIL import Image\n","import numpy as np\n","\n","## 資料目錄來源 以資料夾當作分類 , e.g. Datasets/cats, Datasets/dogs, ...\n","src_dir = r'Datasets/'\n","## 單次預測圖片\n","predict_img = r'Datasets/test2.jpg'\n","\n","## 影像讀取處理\n","datagen = ImageDataGenerator(validation_split=0.2, preprocessing_function=preprocess_input)\n","train_generator = datagen.flow_from_directory(src_dir, target_size=(224, 224), batch_size=20, subset='training')\n","valid_generator = datagen.flow_from_directory(src_dir, target_size=(224, 224), batch_size=20, subset='validation')\n","\n","## 模型建立\n","mobilenetV2 = MobileNetV2(include_top=False, pooling='avg')\n","for mlayer in mobilenetV2.layers:\n","    mlayer.trainable = False\n","mobilenetV2output = mobilenetV2.layers[-1].output\n","fc = layers.Dense(units=train_generator.num_classes, activation='softmax', name='custom_fc') (mobilenetV2output)\n","classification_model = Model(\n","            inputs=mobilenetV2.inputs,\n","            outputs=fc)\n","classification_model.compile(loss='categorical_crossentropy', optimizer='Adam')\n","\n","## 模型訓練\n","history = classification_model.fit(train_generator, epochs=10, validation_data=valid_generator)\n","\n","## 單次預測\n","true_labels_dict = {}\n","for key in train_generator.class_indices:\n","    true_labels_dict[train_generator.class_indices[key]] = key\n","\n","def pred(img_path):\n","    img = preprocess_input(np.array(Image.open(img_path).convert('RGB')))\n","    img = np.array([img])\n","    result_prob = classification_model.predict(img).tolist()[0]\n","    max_index = result_prob.index(max(result_prob))\n","    print(true_labels_dict[max_index], result_prob[max_index])\n","\n","pred(predict_img)\n"]},{"cell_type":"code","source":["from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import Model, layers\n","from PIL import Image\n","import numpy as np\n","\n","## 資料目錄來源 以資料夾當作分類 , e.g. Datasets/cats, Datasets/dogs, ...\n","src_dir = r'Datasets/'\n","## 單次預測圖片\n","predict_img = r'Datasets/test2.jpg'\n","\n","## 影像讀取處理\n","datagen = ImageDataGenerator(validation_split=0.2, preprocessing_function=preprocess_input)\n","train_generator = datagen.flow_from_directory(src_dir, target_size=(224, 224), batch_size=20, subset='training')\n","valid_generator = datagen.flow_from_directory(src_dir, target_size=(224, 224), batch_size=20, subset='validation')\n","\n","## 模型建立\n","mobilenetV2 = MobileNetV2(include_top=False, pooling='avg')\n","for mlayer in mobilenetV2.layers:\n","    mlayer.trainable = False\n","mobilenetV2output = mobilenetV2.layers[-1].output\n","fc = layers.Dense(units=train_generator.num_classes, activation='softmax', name='custom_fc') (mobilenetV2output)\n","classification_model = Model(\n","            inputs=mobilenetV2.inputs,\n","            outputs=fc)\n","classification_model.compile(loss='categorical_crossentropy', optimizer='Adam')\n","\n","## 模型訓練\n","history = classification_model.fit(train_generator, epochs=10, validation_data=valid_generator)\n","\n","## 單次預測\n","true_labels_dict = {}\n","for key in train_generator.class_indices:\n","    true_labels_dict[train_generator.class_indices[key]] = key\n","\n","def pred(img_path):\n","    img = preprocess_input(np.array(Image.open(img_path).convert('RGB')))\n","    img = np.array([img])\n","    result_prob = classification_model.predict(img).tolist()[0]\n","    max_index = result_prob.index(max(result_prob))\n","    print(true_labels_dict[max_index], result_prob[max_index])\n","\n","pred(predict_img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jT-gag_tEkP","executionInfo":{"status":"ok","timestamp":1718418207925,"user_tz":-480,"elapsed":24594,"user":{"displayName":"杏仁Kimi","userId":"06196007917832630822"}},"outputId":"61cc6704-c81e-49af-e04e-45fbeb324f46"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 22 images belonging to 4 classes.\n","Found 4 images belonging to 4 classes.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","2/2 [==============================] - 6s 2s/step - loss: 1.1045 - val_loss: 1.0993\n","Epoch 2/10\n","2/2 [==============================] - 2s 2s/step - loss: 0.6549 - val_loss: 0.8907\n","Epoch 3/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.4931 - val_loss: 0.7353\n","Epoch 4/10\n","2/2 [==============================] - 1s 192ms/step - loss: 0.4157 - val_loss: 0.6034\n","Epoch 5/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.2702 - val_loss: 0.4862\n","Epoch 6/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.1875 - val_loss: 0.3742\n","Epoch 7/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.1258 - val_loss: 0.2798\n","Epoch 8/10\n","2/2 [==============================] - 1s 222ms/step - loss: 0.0987 - val_loss: 0.2118\n","Epoch 9/10\n","2/2 [==============================] - 2s 1s/step - loss: 0.0603 - val_loss: 0.1673\n","Epoch 10/10\n","2/2 [==============================] - 2s 443ms/step - loss: 0.0508 - val_loss: 0.1351\n","1/1 [==============================] - 1s 870ms/step\n","Shark 0.9956888556480408\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import Model, layers\n","from PIL import Image\n","import numpy as np\n","\n","## 資料目錄來源 以資料夾當作分類 , e.g. Datasets/cats, Datasets/dogs, ...\n","src_dir = r'Datasets/'\n","## 單次預測圖片\n","predict_img = r'Datasets/test3.jpg'\n","\n","## 影像讀取處理\n","datagen = ImageDataGenerator(validation_split=0.2, preprocessing_function=preprocess_input)\n","train_generator = datagen.flow_from_directory(src_dir, target_size=(224, 224), batch_size=20, subset='training')\n","valid_generator = datagen.flow_from_directory(src_dir, target_size=(224, 224), batch_size=20, subset='validation')\n","\n","## 模型建立\n","mobilenetV2 = MobileNetV2(include_top=False, pooling='avg')\n","for mlayer in mobilenetV2.layers:\n","    mlayer.trainable = False\n","mobilenetV2output = mobilenetV2.layers[-1].output\n","fc = layers.Dense(units=train_generator.num_classes, activation='softmax', name='custom_fc') (mobilenetV2output)\n","classification_model = Model(\n","            inputs=mobilenetV2.inputs,\n","            outputs=fc)\n","classification_model.compile(loss='categorical_crossentropy', optimizer='Adam')\n","\n","## 模型訓練\n","history = classification_model.fit(train_generator, epochs=10, validation_data=valid_generator)\n","\n","## 單次預測\n","true_labels_dict = {}\n","for key in train_generator.class_indices:\n","    true_labels_dict[train_generator.class_indices[key]] = key\n","\n","def pred(img_path):\n","    img = preprocess_input(np.array(Image.open(img_path).convert('RGB')))\n","    img = np.array([img])\n","    result_prob = classification_model.predict(img).tolist()[0]\n","    max_index = result_prob.index(max(result_prob))\n","    print(true_labels_dict[max_index], result_prob[max_index])\n","\n","pred(predict_img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z_ArEMuutd5z","executionInfo":{"status":"ok","timestamp":1718418378503,"user_tz":-480,"elapsed":30200,"user":{"displayName":"杏仁Kimi","userId":"06196007917832630822"}},"outputId":"b112dd28-5b7e-4e9a-f2fa-1cca6928ae8c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 22 images belonging to 4 classes.\n","Found 4 images belonging to 4 classes.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","2/2 [==============================] - 7s 1s/step - loss: 1.4847 - val_loss: 0.6752\n","Epoch 2/10\n","2/2 [==============================] - 1s 243ms/step - loss: 1.0169 - val_loss: 0.4848\n","Epoch 3/10\n","2/2 [==============================] - 2s 2s/step - loss: 0.6290 - val_loss: 0.3685\n","Epoch 4/10\n","2/2 [==============================] - 2s 200ms/step - loss: 0.5248 - val_loss: 0.2749\n","Epoch 5/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.3363 - val_loss: 0.2156\n","Epoch 6/10\n","2/2 [==============================] - 1s 211ms/step - loss: 0.2856 - val_loss: 0.1628\n","Epoch 7/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.1847 - val_loss: 0.1264\n","Epoch 8/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.1386 - val_loss: 0.0927\n","Epoch 9/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.1021 - val_loss: 0.0659\n","Epoch 10/10\n","2/2 [==============================] - 1s 1s/step - loss: 0.0737 - val_loss: 0.0491\n","1/1 [==============================] - 1s 1s/step\n","Eagle 0.9202722311019897\n"]}]}]}